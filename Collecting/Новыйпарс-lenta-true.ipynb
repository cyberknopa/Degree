{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npai3A4xfJbQ",
        "outputId": "3a0d3953-3a13-413f-afab-ce6ccc995691",
        "pycharm": {
          "is_executing": true
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newscatcher\n",
            "  Downloading newscatcher-0.2.0-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from newscatcher) (2.23.0)\n",
            "Collecting feedparser<6.0.0,>=5.2.1\n",
            "  Downloading feedparser-5.2.1.zip (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting tldextract<3.0.0,>=2.2.2\n",
            "  Downloading tldextract-2.2.3-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.23.0->newscatcher) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.23.0->newscatcher) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.23.0->newscatcher) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.23.0->newscatcher) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from requests-file>=1.4->tldextract<3.0.0,>=2.2.2->newscatcher) (1.15.0)\n",
            "Building wheels for collected packages: feedparser\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-py3-none-any.whl size=44951 sha256=b3bea49a5b16de7a5d5b2d93074689af7ece7705b724d9095abad5807e82a28f\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/38/07/59c89c3d334e7f1f743898af6d21c15ecb3588ad04af7ddee0\n",
            "Successfully built feedparser\n",
            "Installing collected packages: requests-file, tldextract, feedparser, newscatcher\n",
            "Successfully installed feedparser-5.2.1 newscatcher-0.2.0 requests-file-1.5.1 tldextract-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n",
        "data_lenta = pd.read_csv(\"1.csv\")\n",
        "print(data_lenta.head())\n",
        "links_lenta=[]\n",
        "data_lenta.info()\n",
        "links_lenta = data_lenta[\"link\"]\n",
        "f = open('lenta_clean-1.csv', 'w')\n",
        "writer = csv.writer(f)\n",
        "for i in range(0,len(links_lenta)):\n",
        "   current_page = 1\n",
        "   url = links_lenta[i]\n",
        "   req = requests.get(url)\n",
        "   page_content = BeautifulSoup(req.content, 'html.parser')\n",
        "   post_url = links_lenta[i]\n",
        "   post_req = requests.get(post_url)\n",
        "   post = BeautifulSoup(post_req.content, 'html.parser')\n",
        "   post_content = post.find(\"p\", {\"class\": \"topic-body__content-text\"})\n",
        "   post_title=post.find(\"span\", {\"class\": \"topic-body__title\"})\n",
        "   if post_content:\n",
        "                post_content = post_content.text.replace('\\n', '').replace(u'\\xa0', ' ').replace('\"', '“').strip()\n",
        "   else:\n",
        "                failed_news.append(post_url)\n",
        "   print(i)\n",
        "   writer.writerow([post_content,'+', post_url,'+', post_title])"
      ],
      "metadata": {
        "id": "f106GZ2z8R3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
